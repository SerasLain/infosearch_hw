{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y56NU0SrQl7h"
   },
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEp0mmaUsOys"
   },
   "source": [
    "Скачаем это все сюда, потому что в оперативку моего ноута оно не влезло."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqPm88bnLnjn"
   },
   "source": [
    "## Подготовка к запуску в колабе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "V_HC_oRXRMWk",
    "outputId": "f6802754-8141-47a1-d19e-9baf6f6ec971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-02 19:04:02--  http://vectors.nlpl.eu/repository/11/181.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2622716217 (2.4G) [application/zip]\n",
      "Saving to: ‘181.zip’\n",
      "\n",
      "181.zip             100%[===================>]   2.44G  67.4MB/s    in 49s     \n",
      "\n",
      "2019-10-02 19:04:51 (50.9 MB/s) - ‘181.zip’ saved [2622716217/2622716217]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'http://vectors.nlpl.eu/repository/11/181.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "p3gtyYR9qMIO",
    "outputId": "a7aa1298-2e07-4f25-a103-fc4786aadddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  181.zip\n",
      "  inflating: /content/fasttext/meta.json  \n",
      "  inflating: /content/fasttext/model.model  \n",
      "  inflating: /content/fasttext/model.model.vectors_ngrams.npy  \n",
      "  inflating: /content/fasttext/model.model.vectors.npy  \n",
      "  inflating: /content/fasttext/model.model.vectors_vocab.npy  \n",
      "  inflating: /content/fasttext/README  \n"
     ]
    }
   ],
   "source": [
    "!unzip '181.zip' -d 'fasttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXxO3JHysqpI"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "fast_model = '/content/fasttext/model.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6liVV5zRs4eS",
    "outputId": "d99116f6-0084-4833-d1f3-e46850227ce6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load(fast_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OkTcFVmLxmg"
   },
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "SKUKYXej3Y4Z",
    "outputId": "370ed0bd-d4cd-4072-f20b-109320f7cc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "WHVcdUA2Paib",
    "outputId": "31ec5c14-77f8-410a-e631-57a1da093b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\r",
      "\u001b[K     |███████                         | 10kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 20kB 6.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 30kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 40kB 5.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 13.6MB/s \n",
      "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4FSKnv2yuuQ"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5ysQQAB9_72_",
    "outputId": "38ecb559-1565-49dc-9f9e-8ea7efae8ddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['я', 'так', 'большой', 'не', 'мочь']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing('Я так больше не могу', del_stopwords=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNDIUCyYL9Jk"
   },
   "source": [
    "## Получение вектора документа из модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Jsr7DzVL7iR"
   },
   "source": [
    "Проверка на наличие в модели не работает: падает с ошибкой AttributeError, поэтому будем обрабатывать исключение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "1sJbfePZBFD2",
    "outputId": "afa936b6-496e-482f-cb26-365ee7cf7417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fxAhZcYMwwp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lookup(doc, wv):\n",
    "    \"\"\"\n",
    "    Checks if all words in model and returns a vector\n",
    "\n",
    "    :param doc: text string\n",
    "    :param wv: model.wv, WordVectors\n",
    "    :return: vector of the document\n",
    "    \"\"\"\n",
    "    d = preprocessing(doc, del_stopwords=False)\n",
    "    checked = []\n",
    "\n",
    "    for word in d:\n",
    "        try:\n",
    "            word in wv\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        checked.append(wv[word])\n",
    "\n",
    "    vec = np.mean(checked, axis=0)\n",
    "    return vec \n",
    "\n",
    "\n",
    "def cos_sim(v1, v2):\n",
    "    \"\"\"Counts cosine similarity between two vectors\"\"\"\n",
    "    return np.inner(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "\n",
    "def get_dist(text1, text2, wv):\n",
    "    \"\"\"\n",
    "    Counts distanse between two documents\n",
    "    :param text1: str\n",
    "    :param text2: str\n",
    "    :param wv: model.wv, WordVectors\n",
    "\n",
    "    \"\"\"\n",
    "    t1 = lookup(text1, wv)\n",
    "    t2 = lookup(text2, wv)\n",
    "    dist = cos_sim(t1, t2)\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhJH1eSTMY3r"
   },
   "source": [
    "Пример работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oudRV3HBcp4a",
    "outputId": "18b95d30-f065-4a2a-eef6-b9a3e6256cae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793653"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = 'Я ничего не успеваю'\n",
    "t2 = 'Ничего не будет хорошо'\n",
    "get_dist(t1, t2, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIgUfYrLMvM3"
   },
   "source": [
    "## Индексирование корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKaKBYrh2Tct"
   },
   "source": [
    "Дальше я загрузила очищенный корпус Quora question pairs из предыдущей домашки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugYHjwDw2SPd"
   },
   "outputs": [],
   "source": [
    "corpus = 'quora_question_pairs_rus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "M0k98Rx3QrS0",
    "outputId": "d9c82260-842b-4e0b-db8e-dcf401ce394e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",question1,question2,is_duplicate\n",
      "0,Какова история кохинор кох-и-ноор-бриллиант,\"что произойдет, если правительство Индии украдет кохинор кох-и-ноор-алмаз назад\",0\n",
      "1,\"как я могу увеличить скорость моего интернет-соединения, используя vpn\",как повысить скорость интернета путем взлома через dns,0\n",
      "2,\"п\n"
     ]
    }
   ],
   "source": [
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    print(f.read(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIPtE24X5Xoa"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_data(corpus, wv, stop=5000):\n",
    "    \"\"\"\n",
    "    :param corpus: path to csv file with corpus\n",
    "    :param wv: model.KeyedVectors.wv, WordVectors\n",
    "    :param stop: int, how many lines we want to get\n",
    "\n",
    "    :return: \n",
    "        indexed -> list of document vectors\n",
    "        id_to_text -> dict, map of text_id to raw text. \n",
    "        query_to_dupl -> dict, query:id of its duplicate\n",
    "\n",
    "    \"\"\"\n",
    "    indexed = []\n",
    "    id_to_text = {}\n",
    "    query_to_dupl_id = {}\n",
    "    counter = 0\n",
    "\n",
    "    with open(corpus, 'r', encoding='utf-8') as f:\n",
    "        r = csv.reader(f)\n",
    "        for line in r:\n",
    "\n",
    "            if line[0] == '':\n",
    "                continue\n",
    "\n",
    "            _id, text, query, isduplicate = line\n",
    "            id_to_text[_id] = text\n",
    "\n",
    "            if isduplicate == '1':\n",
    "                query_to_dupl_id[query] = _id\n",
    "                \n",
    "            indexed.append(lookup(text, wv))\n",
    "                \n",
    "            counter += 1\n",
    "            if counter >= stop:\n",
    "                break\n",
    "    return indexed, id_to_text, query_to_dupl_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3Y8lMGZg2QND",
    "outputId": "788c847e-909a-410e-c9f4-6c9cf81b6216"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 1.85 s, total: 2min 6s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "indexed, id_to_text, query_to_dupl_id = get_data(corpus, wv, stop=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUP-xGjt4yTY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Indexed_FT.pickle\", 'wb') as f:\n",
    "    pickle.dump((indexed, id_to_text, query_to_dupl_id), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbCTQskH7oV-"
   },
   "outputs": [],
   "source": [
    "with open(\"Indexed_FT.pickle\", 'rb') as f:\n",
    "    indexed, id_to_text, query_to_dupl_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3M9KHJDaWnZ"
   },
   "source": [
    "## Поиск в корпусе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Fy0MyuqSFLi"
   },
   "outputs": [],
   "source": [
    "def search(query, wv, indexed):\n",
    "    \"\"\"\n",
    "    Search tool\n",
    "\n",
    "    :param query: str\n",
    "    :param wv: model.KeyedVectors.wv\n",
    "    :param indexed: iterable of vectors\n",
    "    \"\"\"\n",
    "\n",
    "    query_v = lookup(query, wv)\n",
    "    \n",
    "    result = {}\n",
    "    for i, doc_vector in enumerate(indexed):\n",
    "        score =  cos_sim(query_v, doc_vector)\n",
    "        if type(score) is np.float32:\n",
    "            result[i] = score\n",
    "    \n",
    "    return sorted(result.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "YhjeKSiRVpiu",
    "outputId": "0302303a-157f-47db-ff9a-0f168cab2199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 592 ms, sys: 3.99 ms, total: 596 ms\n",
      "Wall time: 601 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6, 0.86064434),\n",
       " (14150, 0.8143773),\n",
       " (34884, 0.7928043),\n",
       " (21972, 0.79240125),\n",
       " (34574, 0.7921281)]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search('Не хочу быть лингвистом, хочу быть хорошим геологом', wv, indexed)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ey8UrlkxZoat",
    "outputId": "e481782b-dbd6-44ad-d29b-c57f2f196f24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как я могу быть хорошим геологом?'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_text['6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2i3e9DzKaMHv"
   },
   "source": [
    "## Оценка качества поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEm5pkv0aQI8"
   },
   "outputs": [],
   "source": [
    "def get_score(indexed, query_to_dupl_id, wv, test=100):\n",
    "    \"\"\"\n",
    "    Counts the quality of the search (from 0 to 1.0)\n",
    "    \"\"\"\n",
    "    test_query = list(query_to_dupl_id.keys())\n",
    "\n",
    "    if test != 0:\n",
    "        test_query =  test_query[:test]\n",
    "    \n",
    "    test_len = len(test_query)\n",
    "    counter = 0\n",
    "\n",
    "    for q in test_query:\n",
    "        dupl_id = int(query_to_dupl_id[q])\n",
    "\n",
    "        results = search(q, wv, indexed)[:5]\n",
    "        text_ids = [result[0] for result in results]\n",
    "\n",
    "        if dupl_id in text_ids:\n",
    "            counter += 1\n",
    "    \n",
    "    return counter / test_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCnQl2XJIJGe"
   },
   "source": [
    "### Оценка качества поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "zxLBtzZscXID",
    "outputId": "bac3918f-92ce-4193-d77c-00a335f4adb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.1 s, sys: 35.9 ms, total: 58.2 s\n",
      "Wall time: 58.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_score(indexed, query_to_dupl_id, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJERTHKhIPR5"
   },
   "source": [
    "Не то чтобы супер (а на 5000 в корпусе около 70% было т.т)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urrZTOXc1yvP"
   },
   "source": [
    "# ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgSUp4S412l2"
   },
   "source": [
    "Скачаем модель ELMo и распакуем ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "5iJ1Jzkb9Whq",
    "outputId": "7a92e1a3-eec9-42b8-a276-b2d0b1a677e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-05 07:31:12--  http://vectors.nlpl.eu/repository/11/196.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 206986345 (197M) [application/zip]\n",
      "Saving to: ‘196.zip’\n",
      "\n",
      "196.zip             100%[===================>] 197.40M  23.1MB/s    in 13s     \n",
      "\n",
      "2019-10-05 07:31:26 (14.7 MB/s) - ‘196.zip’ saved [206986345/206986345]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://vectors.nlpl.eu/repository/11/196.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "4FzGzqws9i8m",
    "outputId": "58fd7f32-ce74-483d-a4d2-06e317e73372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  196.zip\n",
      "  inflating: /content/ELMO/meta.json  \n",
      "  inflating: /content/ELMO/model.hdf5  \n",
      "  inflating: /content/ELMO/options.json  \n",
      "  inflating: /content/ELMO/README    \n",
      "  inflating: /content/ELMO/vocab.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip '196.zip' -d 'ELMO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-S7w6mK1y0B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('bilm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NSelvo8LMBs"
   },
   "source": [
    "## ELMo helpers from rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOZLtEUwYbx3"
   },
   "source": [
    "В моей домашке они немного подправлены: изменено расширение файла со словарем, чтобы соответствовать модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zW5wZQP_LKwF"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from elmo_helpers import tokenize, get_elmo_vectors, load_elmo_embeddings\n",
    "\n",
    "tf.reset_default_graph()\n",
    "elmo_path = 'ELMO'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJRMPs6hyl8a"
   },
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "colab_type": "code",
    "id": "FZrJc1UWRGmy",
    "outputId": "8cf498eb-9b2d-478f-e7bb-bbfbfc573823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/elmo_helpers.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:276: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:333: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:378: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:522: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /content/bilm/model.py:566: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:567: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f230412b3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f230412b3c8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f230412b3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f230412b3c8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/bilm/model.py:591: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /content/bilm/model.py:536: The name tf.nn.rnn_cell.ResidualWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.ResidualWrapper instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2304060908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2304060908>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2304060908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2304060908>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2330d064e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2330d064e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2330d064e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2330d064e0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2303f02b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2303f02b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2303f02b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f2303f02b70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /content/bilm/elmo.py:92: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batcher, sentence_character_ids, elmo_sentence_input = load_elmo_embeddings(elmo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VF9i5TCK2bUY"
   },
   "source": [
    "## Предобработка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhqA7D6t2wGq"
   },
   "outputs": [],
   "source": [
    "corpus = 'quora_question_pairs_rus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvYDYvnj2Qwq"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_data_elmo(corpus, stop=5000):\n",
    "    \"\"\"\n",
    "    Проходит по корпусу и токенизирует тексты.\n",
    "\n",
    "    :param corpus: path to csv file with corpus\n",
    "    :param stop: int, how many lines we want to get\n",
    "    :return: \n",
    "        indexed -> list of list of strings\n",
    "        id_to_text -> dict, map of text_id to raw text. \n",
    "        query_to_dupl -> dict, query:id of its duplicate\n",
    "\n",
    "    \"\"\"\n",
    "    indexed = []\n",
    "    id_to_text = {}\n",
    "    query_to_dupl_id = {}\n",
    "    counter = 0\n",
    "\n",
    "    with open(corpus, 'r', encoding='utf-8') as f:\n",
    "        r = csv.reader(f)\n",
    "        for line in r:\n",
    "\n",
    "            if line[0] == '':\n",
    "                continue\n",
    "\n",
    "            _id, text, query, isduplicate = line\n",
    "            id_to_text[_id] = text\n",
    "\n",
    "            if isduplicate == '1':\n",
    "                query_to_dupl_id[query] = _id\n",
    "                \n",
    "            indexed.append(tokenize(text))\n",
    "                \n",
    "            counter += 1\n",
    "            if counter >= stop:\n",
    "                break\n",
    "    return indexed, id_to_text, query_to_dupl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZDh7Gwj3AHw"
   },
   "outputs": [],
   "source": [
    "cleaned, id_to_text, query_to_dupl_id = get_data_elmo(corpus, stop=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tX54N0Uy8d67"
   },
   "outputs": [],
   "source": [
    "def crop_vec(vect, sent):\n",
    "    \"\"\"\n",
    "    Crops dummy values\n",
    "\n",
    "    :param vect: np.array, vector from ELMo\n",
    "    :param sent: list of str, tokenized sentence\n",
    "    :return: np.array\n",
    "\n",
    "    \"\"\"\n",
    "    cropped_vector = vect[:len(sent), :]\n",
    "    cropped_vector = np.mean(cropped_vector, axis=0)\n",
    "    return cropped_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwmSNl7b35Ct"
   },
   "outputs": [],
   "source": [
    "def indexing(cleaned, batcher, sentence_character_ids, elmo_sentence_input):\n",
    "    \"\"\" \n",
    "    Indexing corpus\n",
    "    :param cleaned: list if lists of str, tokenized documents from the corpus\n",
    "    :param batcher, sentence_character_ids, elmo_sentence_input: ELMo model\n",
    "\n",
    "    :return: matrix of document vectors\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        # It is necessary to initialize variables once before running inference.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        indexed = []\n",
    "        for i in range(200, len(cleaned)+1, 200):\n",
    "            sentences = cleaned[i-200 : i]\n",
    "            elmo_vectors = get_elmo_vectors(\n",
    "                sess, sentences, batcher, sentence_character_ids, elmo_sentence_input)\n",
    "\n",
    "            for vect, sent in zip(elmo_vectors, sentences):\n",
    "                cropped_vector = crop_vec(vect, sent)\n",
    "                indexed.append(cropped_vector)\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16zKtrbpT8aI"
   },
   "source": [
    "Индексируем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nqXut9fYJx-p",
    "outputId": "2c4f2da6-103c-4a28-9a18-847c17f386a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n",
      "Sentences in this batch: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Затрачено секунд:  5569.673327922821\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "indexed = indexing(cleaned, batcher, sentence_character_ids, elmo_sentence_input)\n",
    "print('Затрачено секунд: ', time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ps4mKFb2UNv6"
   },
   "source": [
    "Сохраним индексированный корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5i6wg8iL_3Ty"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Indexed_ELMO.pickle', 'wb') as f:\n",
    "    pickle.dump((indexed, id_to_text, query_to_dupl_id), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2kzrO3c1yrr"
   },
   "source": [
    "## Поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4r-r_UlEZfa"
   },
   "outputs": [],
   "source": [
    "# Эта функция есть выше в разделе fasttext, но продублируем на всякий случай\n",
    "def cos_sim(v1, v2):\n",
    "    \"\"\"Counts cosine similarity between two vectors\"\"\"\n",
    "    return np.inner(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JXVvi-3JDzJ"
   },
   "outputs": [],
   "source": [
    "# Loading indexed corpus\n",
    "import pickle\n",
    "\n",
    "with open('Indexed_ELMO.pickle', 'rb') as f:\n",
    "    indexed, id_to_text, query_to_dupl_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_pXt2ZNKtgr"
   },
   "outputs": [],
   "source": [
    "def prepare_query(query, batcher, sentence_character_ids, elmo_sentence_input):\n",
    "    \"\"\" \n",
    "    Gets vector of query\n",
    "\n",
    "    :param query: str\n",
    "    :param batcher, sentence_character_ids, elmo_sentence_input: ELMo model\n",
    "    \n",
    "    :return: vector of query\n",
    "    \"\"\"\n",
    "    q = [tokenize(query)]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vector = crop_vec(get_elmo_vectors(sess, q, batcher,\n",
    "                                           sentence_character_ids,\n",
    "                                           elmo_sentence_input)[0], q[0])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idUTLGc9ME4-"
   },
   "outputs": [],
   "source": [
    "def search_tool_elmo(query, batcher, sentence_character_ids,\n",
    "                     elmo_sentence_input, indexed):\n",
    "    \"\"\"\n",
    "    Search query in corpus\n",
    "\n",
    "    :param: query: str\n",
    "    :param batcher, sentence_character_ids, elmo_sentence_input: ELMo model\n",
    "    :param indexed: np.array, matrix of indexed corpus\n",
    "\n",
    "    :return: list, sorted results\n",
    "    \"\"\"\n",
    "    q = prepare_query(query, batcher, sentence_character_ids, \n",
    "                      elmo_sentence_input)\n",
    "\n",
    "    result = {}\n",
    "    for i, doc_vector in enumerate(indexed):\n",
    "        score =  cos_sim(q, doc_vector)\n",
    "        if type(score) is np.float32:\n",
    "            result[i] = score\n",
    "    \n",
    "    return sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBmnPhR6WWbC"
   },
   "outputs": [],
   "source": [
    "Пример поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "W7zAtMQxNPwt",
    "outputId": "c951e03c-d31f-4963-9a06-c4ed099f0626"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences in this batch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0.7573212), (500, 0.7192065), (427, 0.6865775), (363, 0.6859282), (544, 0.6816523)]\n"
     ]
    }
   ],
   "source": [
    "res = search_tool_elmo('Что нужно, чтобы стать хорошим геологом?', batcher,\n",
    "                       sentence_character_ids, elmo_sentence_input, indexed)\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0YB-1yAoOuIm",
    "outputId": "0e6c4f7b-7dcf-4f6d-997b-8b6c28536612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как я могу быть хорошим геологом?'"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_text['6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMuPsrAVO2bu"
   },
   "source": [
    "## Оценка качества поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nKyi68kO5CH"
   },
   "outputs": [],
   "source": [
    "def get_score_elmo(indexed, query_to_dupl_id, batcher, sentence_character_ids, \n",
    "                   elmo_sentence_input, test=100):\n",
    "    \"\"\"\n",
    "    Counts the quality of the search (from 0 to 1.0)\n",
    "    \"\"\"\n",
    "    test_query = list(query_to_dupl_id.keys())\n",
    "\n",
    "    if test != 0:\n",
    "        test_query =  test_query[:test]\n",
    "    \n",
    "    test_len = len(test_query)\n",
    "    counter = 0\n",
    "\n",
    "    for q in test_query:\n",
    "        dupl_id = int(query_to_dupl_id[q])\n",
    "\n",
    "        results = search_tool_elmo(q, batcher, sentence_character_ids,\n",
    "                                   elmo_sentence_input, indexed)[:5]\n",
    "        text_ids = [result[0] for result in results]\n",
    "\n",
    "        if dupl_id in text_ids:\n",
    "            counter += 1\n",
    "    \n",
    "    return counter / test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0fInRcHBPs2L",
    "outputId": "cd9bedb9-c7d1-47c4-bd28-8d03c320c261"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n",
      "Sentences in this batch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n",
      "Затрачено времени 290.902761220932\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(get_score_elmo(indexed,query_to_dupl_id, batcher, sentence_character_ids, \n",
    "                   elmo_sentence_input))\n",
    "print('Затрачено времени', time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97kFtbjcp5vl"
   },
   "source": [
    "М-да. Лучше не стало.\n",
    "\n",
    "Но на небольших корпусах дает результаты лучше. Возможно, это какие-то ограничения моей функции для подсчета результатов :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilLbRkZuP65J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw3_w2v.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
